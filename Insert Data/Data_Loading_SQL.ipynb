{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ea42a6-6a28-4f68-a9d1-6bf606a8dedb",
   "metadata": {},
   "source": [
    "#### 1) Setup, Configuration, and Database Engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d96fcf0-e27b-471b-be3f-93b62f20341d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection successful to database: C4_Project\n"
     ]
    }
   ],
   "source": [
    "### 1. Install Libraries, Configuration, and Database Engine ###\n",
    "\n",
    "#!pip install sqlalchemy pandas psycopg2-binary\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from sqlalchemy import create_engine, text \n",
    "from datetime import datetime\n",
    "\n",
    "# --- DATABASE CONFIGURATION ---\n",
    "\n",
    "DB_USER = \"postgres\"  \n",
    "DB_PASSWORD = \"123\" \n",
    "DB_HOST = \"localhost\" \n",
    "DB_PORT = \"5432\"\n",
    "DB_NAME = \"C4_Project\" \n",
    "\n",
    "DATABASE_URL = f\"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "# 2. File Directory \n",
    "OUTPUT_DIR = '/Users/mayapatel/Desktop/SQL/Project/Check Point 4/Simulated Data Files'\n",
    "\n",
    "# --- SQLAlchemy Engine ---\n",
    "try:\n",
    "    # Create the connection engine.\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "    \n",
    "    # Test connection\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT current_database();\")).scalar()\n",
    "        print(f\"Connection successful to database: {result}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"FATAL CONNECTION ERROR: Check if your PostgreSQL server is running (via PgAdmin) and confirm the password/port: {e}\")\n",
    "\n",
    "\n",
    "# --- Utility Function for Loading ---\n",
    "def load_data_from_csv(table_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV, applies type conversion, and loads data into the specified table \n",
    "    in the 'realestate' schema using pandas.to_sql.\n",
    "    \"\"\"\n",
    "    csv_file = os.path.join(OUTPUT_DIR, f\"{table_name}.csv\")\n",
    "    print(f\"-> Processing {table_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV. \n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # --- Data Type Coercion ---\n",
    "        date_cols = ['hire_date', 'listed_date', 'closed_date', 'offer_date', 'start_date', 'end_date', 'close_date']\n",
    "        datetime_cols = ['created_at', 'appointment_datetime']\n",
    "        \n",
    "        for col in date_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d').dt.date\n",
    "        \n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        if 'demographics' in df.columns:\n",
    "            df['demographics'] = df['demographics'].apply(lambda x: json.loads(x) if pd.notna(x) else None)\n",
    "        \n",
    "        # --- Execute Bulk Insert ---\n",
    "        rows_before = len(df)\n",
    "        df.to_sql(\n",
    "            table_name,\n",
    "            con=engine,\n",
    "            schema='realestate',  \n",
    "            if_exists='append',   \n",
    "            index=False          \n",
    "        )\n",
    "        print(f\"   [SUCCESS] Loaded {rows_before} rows into realestate.{table_name}.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        # Skip temporary ID files\n",
    "        print(f\"   [SKIP] CSV file not found (or intentionally skipped): {table_name}.csv.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [FATAL ERROR] Failed to load {table_name}. Check constraints: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4571fcf5-3015-45ad-bece-f8682ea58260",
   "metadata": {},
   "source": [
    "#### 2) Dependency Order and Execution\n",
    "\n",
    "Cell executes the bulk insertion process following the strict dependency order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a94b740-f193-4e2c-8b09-f6eff12c1497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "STARTING BULK DATA LOAD INTO DATABASE: C4_Project (Fixed)\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Level 1 (Foundation) ---\n",
      "-> Processing marketing_channels...\n",
      "   [SKIP] CSV file not found (or intentionally skipped): marketing_channels.csv.\n",
      "-> Processing addresses...\n",
      "   [SUCCESS] Loaded 150 rows into realestate.addresses.\n",
      "\n",
      "--- Level 2 (Organization) ---\n",
      "-> Processing users...\n",
      "   [SUCCESS] Loaded 100 rows into realestate.users.\n",
      "-> Processing offices...\n",
      "   [SUCCESS] Loaded 10 rows into realestate.offices.\n",
      "\n",
      "--- Level 3 (Personnel) ---\n",
      "-> Processing clients...\n",
      "   [SUCCESS] Loaded 100 rows into realestate.clients.\n",
      "-> Processing agents...\n",
      "   [SUCCESS] Loaded 100 rows into realestate.agents.\n",
      "\n",
      "--- Level 4 (Inventory Prep) ---\n",
      "-> Processing client_preferences...\n",
      "   [SUCCESS] Loaded 80 rows into realestate.client_preferences.\n",
      "-> Processing properties...\n",
      "   [SUCCESS] Loaded 100 rows into realestate.properties.\n",
      "\n",
      "--- Level 5 (Listings) ---\n",
      "-> Processing listings...\n",
      "   [SUCCESS] Loaded 100 rows into realestate.listings.\n",
      "\n",
      "--- Level 6 (Activities) ---\n",
      "-> Processing appointments...\n",
      "   [SUCCESS] Loaded 100 rows into realestate.appointments.\n",
      "-> Processing offers...\n",
      "   [SUCCESS] Loaded 120 rows into realestate.offers.\n",
      "-> Processing transactions...\n",
      "   [SUCCESS] Loaded 100 rows into realestate.transactions.\n",
      "-> Processing marketing_campaigns...\n",
      "   [SUCCESS] Loaded 150 rows into realestate.marketing_campaigns.\n",
      "------------------------------------------------------------\n",
      "DATA LOADING COMPLETE (Attempt 6).\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### 2. Execute Bulk Data Load in Dependency Order (FIXED) ###\n",
    "\n",
    "# List of all tables to be loaded, organized by Foreign Key dependency.\n",
    "TABLE_LOAD_ORDER = {\n",
    "    # LEVEL 1: Only addresses and marketing_channels are left. The other 6 lookups are skipped.\n",
    "    \"Level 1 (Foundation)\": [\n",
    "        # Skipping states, property_types, etc., as data exists from schema INSERTs.\n",
    "        \"marketing_channels\", \"addresses\"\n",
    "    ],\n",
    "    # LEVEL 2: Organization & Users\n",
    "    \"Level 2 (Organization)\": [\n",
    "        \"users\", \"offices\"\n",
    "    ],\n",
    "    # LEVEL 3: Core Personnel (FIXED: Clients load order moved before agents)\n",
    "    # Clients must load first to ensure properties and preferences can reference them.\n",
    "    \"Level 3 (Personnel)\": [\n",
    "        \"clients\", \"agents\"\n",
    "    ],\n",
    "    # LEVEL 4: Inventory Prep\n",
    "    \"Level 4 (Inventory Prep)\": [\n",
    "        \"client_preferences\", \"properties\"\n",
    "    ],\n",
    "    # LEVEL 5: Listings\n",
    "    \"Level 5 (Listings)\": [\n",
    "        \"listings\"\n",
    "    ],\n",
    "    # LEVEL 6: Activities & Transactions\n",
    "    \"Level 6 (Activities)\": [\n",
    "        \"appointments\", \"offers\", \"transactions\", \"marketing_campaigns\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def load_data_from_csv(table_name):\n",
    "    \"\"\"\n",
    "    Reads a CSV, applies type conversion, handles JSONB, and loads data.\n",
    "    \"\"\"\n",
    "    csv_file = os.path.join(OUTPUT_DIR, f\"{table_name}.csv\")\n",
    "    print(f\"-> Processing {table_name}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # --- Data Type Coercion (Includes JSONB fix) ---\n",
    "        date_cols = ['hire_date', 'listed_date', 'closed_date', 'offer_date', 'start_date', 'end_date', 'close_date']\n",
    "        datetime_cols = ['created_at', 'appointment_datetime']\n",
    "        \n",
    "        for col in date_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce', format='%Y-%m-%d').dt.date\n",
    "        \n",
    "        for col in datetime_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        \n",
    "        # **SOLUTION 2 FIX:** Explicitly serialize Python dict/object data for JSONB column\n",
    "        if 'demographics' in df.columns:\n",
    "            df['demographics'] = df['demographics'].apply(lambda x: json.dumps(json.loads(x)) if pd.notna(x) else None)\n",
    "\n",
    "        # **SOLUTION 4 FIX:** Ensure commission_amount aligns with price * rate for trigger\n",
    "        if 'close_price' in df.columns and 'commission_pct' in df.columns and 'commission_amount' in df.columns:\n",
    "            # Re-calculate and aggressively round commission_amount to two decimal places \n",
    "            # to match the expected format used in the trigger check.\n",
    "            df['commission_amount'] = (df['close_price'] * df['commission_pct']).round(2)\n",
    "        \n",
    "        # --- Execute Bulk Insert ---\n",
    "        rows_before = len(df)\n",
    "        df.to_sql(\n",
    "            table_name,\n",
    "            con=engine,\n",
    "            schema='realestate',\n",
    "            if_exists='append',\n",
    "            index=False\n",
    "        )\n",
    "        print(f\"   [SUCCESS] Loaded {rows_before} rows into realestate.{table_name}.\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"   [SKIP] CSV file not found (or intentionally skipped): {table_name}.csv.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [FATAL ERROR] Failed to load {table_name}. Check constraints: {e}\")\n",
    "\n",
    "\n",
    "# --- EXECUTION ---\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"STARTING BULK DATA LOAD INTO DATABASE: {DB_NAME} (Fixed)\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Loop through the levels of dependency\n",
    "for level, table_list in TABLE_LOAD_ORDER.items():\n",
    "    print(f\"\\n--- {level} ---\")\n",
    "    for table_name in table_list:\n",
    "        load_data_from_csv(table_name)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(\"DATA LOADING COMPLETE (Attempt 6).\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fda131-b665-477b-8289-a97444637668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
